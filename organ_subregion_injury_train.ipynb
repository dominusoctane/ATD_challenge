{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90a23e85-c011-4b14-85a0-d13bc5a61072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"  # Use the 3rd and 4th GPU. Indexing starts from 0.\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Use the 3rd and 4th GPU. Indexing starts from 0.\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    # When using GPU\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        # torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "set_seed(3047)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65341c0f-4e8e-4cce-a1b4-263f79a0324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create customdataset that loads in all images for a particular volume, particular series_id\n",
    "# resize/crop the images to 224x224 and shrink/increase them to 15 slices. \n",
    "# crop by reducing the width, so it conforms more like a 1.15 ratio between width and height\n",
    "# Do it for RGB channels\n",
    "\n",
    "# Afterwards assign them labels based on their injury status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abd960e4-bf41-4763-8c83-26eb6eeaff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "def collect_image_paths(base_folder):\n",
    "    \"\"\"\n",
    "    Traverse the directory structure under base_folder and collect paths to all image files.\n",
    "    \n",
    "    Args:\n",
    "    - base_folder (str): The main directory containing all series_id subfolders.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Contains columns 'series_id', 'organ', 'image_path', and 'image_name'.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # Iterate through each series_id in the main folder\n",
    "    for series_id in os.listdir(base_folder):\n",
    "        series_path = os.path.join(base_folder, series_id)\n",
    "        \n",
    "        # Check if it's a directory and not a file\n",
    "        if os.path.isdir(series_path):\n",
    "            \n",
    "            # Iterate through each organ folder under the current series_id\n",
    "            for organ in ['bowel', 'kidneys', 'liver', 'spleen']:\n",
    "                organ_path = os.path.join(series_path, organ)\n",
    "                \n",
    "                # Collect all .jpg image paths under the organ directory\n",
    "                organ_images = [os.path.join(organ_path, fname) for fname in os.listdir(organ_path) if fname.endswith('.jpg')]\n",
    "                \n",
    "                for image_path in organ_images:\n",
    "                    image_name = os.path.basename(image_path).replace('.jpg', '')  # Extract filename without extension\n",
    "                    data.append({\n",
    "                        'series_id': series_id,\n",
    "                        'organ': organ,\n",
    "                        'image_path': image_path,\n",
    "                        'image_name': image_name\n",
    "                    })\n",
    "                    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['series_id'] = df['series_id'].astype(int)\n",
    "    df['image_name'] = df['image_name'].astype(int)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "base_folder = 'series_image_split'\n",
    "all_image_paths = collect_image_paths(base_folder)\n",
    "all_image_paths = all_image_paths.sort_values(by=['series_id','image_name'],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82844488-0c65-45f7-adfb-e2816ba79474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_info = pd.read_csv('train.csv')\n",
    "train_info.patient_id = train_info.patient_id.astype(str)\n",
    "\n",
    "mapping_df = pd.read_csv('patient_series_mapping.csv')\n",
    "mapping_df = mapping_df.drop('Unnamed: 0',axis=1)\n",
    "mapping_df = mapping_df[['patient_id','series_id']]\n",
    "mapping_df['patient_id'] = mapping_df['patient_id'].astype(int)\n",
    "mapping_df['series_id'] = mapping_df['series_id'].astype(int)\n",
    "\n",
    "merged_df = all_image_paths.merge(mapping_df[['series_id', 'patient_id']], on='series_id', how='left')\n",
    "\n",
    "y_original_format = train_info.drop(['bowel_healthy','extravasation_healthy','any_injury'], axis=1)\n",
    "y_original_format['patient_id'] = y_original_format['patient_id'].astype(int)\n",
    "\n",
    "merged_df = merged_df.merge(y_original_format[['patient_id','bowel_injury', 'extravasation_injury', 'kidney_healthy',\n",
    "       'kidney_low', 'kidney_high', 'liver_healthy', 'liver_low', 'liver_high',\n",
    "       'spleen_healthy', 'spleen_low', 'spleen_high']], on='patient_id', how='left')\n",
    "\n",
    "y_original_format = train_info.drop(['patient_id','bowel_healthy','extravasation_healthy','any_injury'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cfbf2b43-fb43-4a4e-82fc-4b436e2fa70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "image_size = 320\n",
    "\n",
    "class OrganTrainDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.series_ids = self.dataframe['series_id'].unique()\n",
    "        # self.organs = ['liver', 'spleen', 'kidneys']\n",
    "        self.organs = ['liver', 'spleen', 'kidneys', 'bowel']\n",
    "        \n",
    "        # Define columns that contain labels\n",
    "        self.label_columns = ['bowel_injury', 'extravasation_injury', 'kidney_healthy', \n",
    "                              'kidney_low', 'kidney_high', 'liver_healthy', 'liver_low', \n",
    "                              'liver_high', 'spleen_healthy', 'spleen_low', 'spleen_high']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.series_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        series_id = self.series_ids[idx]\n",
    "        curr_df = self.dataframe[self.dataframe['series_id'] == series_id]\n",
    "\n",
    "        # 4 organs * 15 depth \n",
    "        images = []\n",
    "        for org in self.organs:\n",
    "            org_df = curr_df[curr_df.organ == org]\n",
    "            org_path = np.array(org_df.image_path)\n",
    "            if len(org_path)==0:\n",
    "                print(series_id)\n",
    "                print('Something wrong')\n",
    "            else:\n",
    "                org_images = []  # Store images for the current organ\n",
    "                for path in org_path:\n",
    "                    image = cv2.imread(path)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    if self.transform:\n",
    "                        image = self.transform(image=image)['image']\n",
    "                    image = image.transpose(2, 0, 1).astype(np.float32) / 255.\n",
    "                    org_images.append(image)\n",
    "                \n",
    "                # Stack images for the current organ and resize depth to 15\n",
    "                org_images = np.stack(org_images, 1)\n",
    "                org_images = torch.tensor(org_images)\n",
    "                resized_org_images = F.interpolate(org_images.unsqueeze(0), size=(10, image_size, image_size), mode='trilinear', align_corners=True).squeeze(0)\n",
    "                # print(resized_org_images.shape)\n",
    "                images.append(resized_org_images)\n",
    "    \n",
    "        # Stack all organs' images together\n",
    "        images = torch.cat(images, 1)\n",
    "        images = images.transpose(1, 0)\n",
    "        \n",
    "        # print(images.shape)\n",
    "        \n",
    "        # Extract labels for the given series_id\n",
    "        labels = curr_df.iloc[0][self.label_columns].values.astype(np.float32)\n",
    "        \n",
    "        return images, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0bea3aed-2305-4c80-8a53-b329489cc433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "573a7c66-93a9-4d9b-b6fe-e9ca2a40d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = albumentations.Compose([\n",
    "    albumentations.RandomBrightnessContrast(contrast_limit=0.2, brightness_limit=0, p=1.0),\n",
    "    albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, \n",
    "                                    rotate_limit=20, border_mode=cv2.BORDER_CONSTANT, p=1.0),\n",
    "    albumentations.CoarseDropout(max_holes=2, max_height=int(0.3*image_size), \n",
    "                          max_width=int(0.3*image_size), fill_value=0, always_apply=True, p=1.0),\n",
    "])\n",
    "\n",
    "\n",
    "transforms_valid = albumentations.Compose([\n",
    "    albumentations.Resize(image_size, image_size),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cdb590ab-0933-4538-a5dd-0adae879be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_organs = 4  # Number of organs\n",
    "n_slice_per_c = 10\n",
    "bs = 1\n",
    "out_dim = 5\n",
    "pretrained = True\n",
    "drop_path_rate = 0\n",
    "drop_rate_last = 0.3\n",
    "drop_rate = 0\n",
    "backbone = 'convnext_nano'\n",
    "use_amp = True\n",
    "\n",
    "in_chans = 3\n",
    "\n",
    "class TimmModelType2(nn.Module):\n",
    "    def __init__(self, backbone, pretrained=False):\n",
    "        super(TimmModelType2, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=out_dim,\n",
    "            features_only=False,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, out_dim),\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):  # (bs, n_slice_per_c * n_organs * in_chans, H, W)\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs * n_slice_per_c * n_organs, in_chans, image_size, image_size)\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(bs, n_slice_per_c * n_organs, -1)\n",
    "        feat1, _ = self.lstm(feat)\n",
    "        feat1 = feat1.contiguous().view(bs * n_slice_per_c * n_organs, 512)\n",
    "        feat2, _ = self.lstm2(feat)\n",
    "        return self.head(feat1), self.head2(feat2[:, 0])\n",
    "\n",
    "class TimmModelWithDualLSTM(nn.Module):\n",
    "    def __init__(self, backbone, pretrained=False):\n",
    "        super(TimmModelWithDualLSTM, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=1,\n",
    "            features_only=False,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        # Determine the dimension of the features from the encoder\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "        # Binary classification LSTM\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=1, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 2)  # 2 binary labels\n",
    "        )\n",
    "\n",
    "        # Multiclass classification LSTM\n",
    "        self.lstm2 = nn.LSTM(hdim, 256, num_layers=1, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 9)  # 9 multiclass labels\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # (bs, n_slice_per_c * n_organs * in_chans, H, W)\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs * n_slice_per_c * n_organs, in_chans, image_size, image_size)\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(bs, n_slice_per_c * n_organs, -1)\n",
    "        \n",
    "        feat1, _ = self.lstm(feat)\n",
    "        feat1 = torch.mean(feat1, dim=1)\n",
    "        #feat1 = feat1.contiguous().view(bs * n_slice_per_c * n_organs, 512)\n",
    "        binary_out = self.head(feat1)\n",
    "              \n",
    "        feat2, _ = self.lstm2(feat)\n",
    "        feat2 = torch.mean(feat2, dim=1)\n",
    "        #feat2 = feat2.contiguous().view(bs * n_slice_per_c * n_organs, 512)\n",
    "        multiclass_out = self.head2(feat2)\n",
    "        \n",
    "        return binary_out, multiclass_out#[0,:], multiclass_out[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2908b118-eb05-4810-bf1f-bd3ab7bc4acb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mixup(input, truth, clip=[0, 1]):\n",
    "    indices = torch.randperm(input.size(0))\n",
    "    shuffled_input = input[indices]\n",
    "    \n",
    "    # Assuming truth is in the shape (batch_size, n_labels)\n",
    "    # where n_labels includes binary and multiclass labels\n",
    "    shuffled_labels = truth[indices]\n",
    "    \n",
    "    lam = np.random.uniform(clip[0], clip[1])\n",
    "    mixed_input = input * lam + shuffled_input * (1 - lam)\n",
    "    \n",
    "    # Mix labels\n",
    "    mixed_labels = truth * lam + shuffled_labels * (1 - lam)\n",
    "    \n",
    "    return mixed_input, mixed_labels, shuffled_labels, lam\n",
    "\n",
    "def binary_criterion(logits, targets, model_device):\n",
    "    # Split logits\n",
    "    bowel_logits = logits[:, 0]\n",
    "    extravasation_logits = logits[:, 1]\n",
    "\n",
    "    # Split targets\n",
    "    bowel_targets = targets[:, 0]\n",
    "    extravasation_targets = targets[:, 1]\n",
    "    \n",
    "    # Binary Loss for bowel and extravasation\n",
    "    bowel_losses = nn.BCEWithLogitsLoss(reduction='none')(bowel_logits, bowel_targets)\n",
    "    bowel_losses[bowel_targets > 0] *= 2.\n",
    "    \n",
    "    extravasation_losses = nn.BCEWithLogitsLoss(reduction='none')(extravasation_logits, extravasation_targets)\n",
    "    extravasation_losses[extravasation_targets > 0] *= 2.\n",
    "\n",
    "    # Combine and normalize the binary losses\n",
    "    all_losses = torch.cat([bowel_losses, extravasation_losses], dim=0)\n",
    "    norm = torch.ones(all_losses.shape[0]).to(model_device)\n",
    "    norm[torch.cat([bowel_targets, extravasation_targets], dim=0) > 0] *= 2\n",
    "    total_loss = all_losses.sum() / norm.sum()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def extravasation_binary_criterion(logits, targets, model_device):\n",
    "    # Split logits\n",
    "    extravasation_logits = logits[:, 1]\n",
    "\n",
    "    # Split targets\n",
    "    extravasation_targets = targets[:, 1]\n",
    "    \n",
    "    # Binary Loss for bowel and extravasation  \n",
    "    extravasation_losses = nn.BCEWithLogitsLoss(reduction='none')(extravasation_logits, extravasation_targets)\n",
    "\n",
    "    # Combine and normalize the binary losses\n",
    "    all_losses = extravasation_losses\n",
    "    norm = torch.ones(all_losses.shape[0]).to(model_device)\n",
    "    norm[extravasation_losses > 0] *= 4\n",
    "    total_loss = all_losses.sum() / norm.sum()\n",
    "\n",
    "    return total_loss\n",
    "    \n",
    "def bowel_binary_criterion(logits, targets, model_device):\n",
    "    # Split logits\n",
    "    bowel_logits = logits[:, 0]\n",
    "\n",
    "    # Split targets\n",
    "    bowel_targets = targets[:, 0]\n",
    "    \n",
    "    # Binary Loss for bowel and extravasation\n",
    "    bowel_losses = nn.BCEWithLogitsLoss(reduction='none')(bowel_logits, bowel_targets)\n",
    "    \n",
    "    # Combine and normalize the binary losses\n",
    "    all_losses = bowel_losses\n",
    "    norm = torch.ones(all_losses.shape[0]).to(model_device)\n",
    "    norm[bowel_targets > 0] *= 4\n",
    "    total_loss = all_losses.sum() / norm.sum()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def compute_organ_criterion(logits, targets, slice_indices, model_device):\n",
    "    organ_logits = logits[:, slice_indices]\n",
    "    organ_targets = targets[:, slice_indices]\n",
    "\n",
    "    # Multiclass Loss\n",
    "    #ce = nn.CrossEntropyLoss(reduction='none', weight = class_weights)\n",
    "    ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    organ_losses = ce(organ_logits, organ_targets)\n",
    "\n",
    "    # Normalization\n",
    "    norm = torch.ones(organ_losses.shape[0]).to(model_device)\n",
    "    organ_targets_flat = organ_targets.argmax(dim=1)\n",
    "    norm[organ_targets_flat > 0] *= 4\n",
    "    \n",
    "    # Calculate the final loss\n",
    "    total_loss = organ_losses.sum() / norm.sum()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def kidney_multi_criterion(logits, targets, model_device):\n",
    "    return compute_organ_criterion(logits, targets, slice(0,3), model_device)\n",
    "\n",
    "def liver_multi_criterion(logits, targets, model_device):\n",
    "    return compute_organ_criterion(logits, targets, slice(3,6), model_device)\n",
    "\n",
    "def spleen_multi_criterion(logits, targets, model_device):\n",
    "    return compute_organ_criterion(logits, targets, slice(6,9), model_device)\n",
    "    \n",
    "def multiclass_criterion(logits, targets, model_device):\n",
    "    # Split logits\n",
    "    print(logits.shape)\n",
    "    \n",
    "    kidney_logits = logits[:, 0:3]  # Classes 0, 1, 2\n",
    "    liver_logits = logits[:, 3:6]   # Classes 3, 4, 5\n",
    "    spleen_logits = logits[:, 6:9]  # Classes 6, 7, 8\n",
    "\n",
    "    # Split targets\n",
    "    kidney_targets = targets[:, 0:3]\n",
    "    liver_targets = targets[:, 3:6]\n",
    "    spleen_targets = targets[:, 6:9]\n",
    "\n",
    "    # Multiclass Losses\n",
    "    ce = nn.CrossEntropyLoss(reduction='none')\n",
    "    kidney_losses = ce(kidney_logits, kidney_targets)\n",
    "    liver_losses = ce(liver_logits, liver_targets)\n",
    "    spleen_losses = ce(spleen_logits, spleen_targets)\n",
    "\n",
    "    # Combine and normalize the multiclass losses\n",
    "    all_losses = torch.cat([kidney_losses, liver_losses, spleen_losses], dim=0)\n",
    "    norm = torch.ones(all_losses.shape[0]*3).to(model_device)\n",
    "\n",
    "    # Reshape each organ's targets\n",
    "    kidney_targets_flat = kidney_targets.reshape(-1)\n",
    "    liver_targets_flat = liver_targets.reshape(-1)\n",
    "    spleen_targets_flat = spleen_targets.reshape(-1)\n",
    "    \n",
    "    # Concatenate the reshaped targets\n",
    "    combined_targets = torch.cat([kidney_targets_flat, liver_targets_flat, spleen_targets_flat])\n",
    "    \n",
    "    # Update the norm based on the targets\n",
    "    norm[combined_targets > 0] *= 2\n",
    "    \n",
    "    # Calculate the final loss\n",
    "    total_loss = all_losses.sum() / norm.sum()\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a0294bd1-0ee2-4447-852a-dbfcc937806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "# Initialize the stratifier\n",
    "stratifier = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd187836-a814-4eac-ba6b-9e77dce53630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 0\n",
      "Processing fold 1\n",
      "Processing fold 2\n",
      "Processing fold 3\n",
      "Processing fold 4\n"
     ]
    }
   ],
   "source": [
    "train_split = []\n",
    "test_split = []\n",
    "for fold, (train_patients, test_patients) in enumerate(stratifier.split(y_original_format, y_original_format)):\n",
    "    print(f\"Processing fold {fold}\")\n",
    "    train_split.append(train_patients)\n",
    "    test_split.append(test_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cdb3c55b-6a7b-4360-93da-67a60ab8fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_organs(organ_series):\n",
    "    required_organs = {\"liver\", \"spleen\", \"kidneys\", \"bowel\"}\n",
    "    return required_organs.issubset(set(organ_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b895b92-a92e-4f84-8d65-07df0829ac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "Mon Oct 16 22:12:23 2023 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                     | 0/39 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Train Loss: 0.3301: 100%|████████████████████████████████████████████████████████| 39/39 [01:42<00:00,  2.62s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 20/20 [00:21<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 1, lr: 0.0000928, train loss: 0.33011, valid loss: 0.31598, metric: 0.315980.\n",
      "metric_best (inf --> 0.315980). Saving model ...\n",
      "Mon Oct 16 22:14:27 2023 Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3306: 100%|████████████████████████████████████████████████████████| 39/39 [01:42<00:00,  2.62s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 20/20 [00:22<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 2, lr: 0.0000741, train loss: 0.33059, valid loss: 0.32259, metric: 0.322587.\n",
      "Mon Oct 16 22:16:31 2023 Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3291: 100%|████████████████████████████████████████████████████████| 39/39 [01:41<00:00,  2.59s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 20/20 [00:22<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 3, lr: 0.0000509, train loss: 0.32912, valid loss: 0.33802, metric: 0.338016.\n",
      "Mon Oct 16 22:18:35 2023 Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3312: 100%|████████████████████████████████████████████████████████| 39/39 [01:41<00:00,  2.61s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 20/20 [00:22<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 4, lr: 0.0000322, train loss: 0.33120, valid loss: 0.33690, metric: 0.336899.\n",
      "Mon Oct 16 22:20:40 2023 Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3256: 100%|████████████████████████████████████████████████████████| 39/39 [01:42<00:00,  2.62s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 20/20 [00:23<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 5, lr: 0.0000250, train loss: 0.32556, valid loss: 0.35337, metric: 0.353374.\n",
      "Using 2 GPUs!\n",
      "Mon Oct 16 22:22:46 2023 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                     | 0/36 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Train Loss: 0.3463: 100%|████████████████████████████████████████████████████████| 36/36 [01:38<00:00,  2.74s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 26/26 [00:24<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1, lr: 0.0000322, train loss: 0.34626, valid loss: 0.31870, metric: 0.318702.\n",
      "metric_best (inf --> 0.318702). Saving model ...\n",
      "Mon Oct 16 22:24:50 2023 Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3434: 100%|████████████████████████████████████████████████████████| 36/36 [01:39<00:00,  2.76s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 26/26 [00:25<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 2, lr: 0.0000741, train loss: 0.34336, valid loss: 0.29303, metric: 0.293033.\n",
      "metric_best (0.318702 --> 0.293033). Saving model ...\n",
      "Mon Oct 16 22:26:55 2023 Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3688:  44%|████████████████████████▉                               | 16/36 [00:46<01:07,  3.38s/it]"
     ]
    }
   ],
   "source": [
    "for fold in range(0,5):\n",
    "    log_dir = 'logs'\n",
    "    model_dir = 'best_lstm_model'\n",
    "    n_epochs = 5\n",
    "    DEBUG = True\n",
    "    p_mixup = 0.5\n",
    "    kernel_type ='not_real'\n",
    "    \n",
    "    log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n",
    "    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    \n",
    "    train_patients, test_patients = train_split[fold], test_split[fold]#train_patients, test_patients = next(stratifier.split(y_original_format, y_original_format))\n",
    "\n",
    "    train_df = merged_df[merged_df['patient_id'].isin(train_patients)]\n",
    "\n",
    "    organ_check = train_df.groupby('series_id')['organ'].agg(check_organs)\n",
    "    missing_organs_series_ids = organ_check[~organ_check].index.tolist()\n",
    "    train_df = train_df[~train_df['series_id'].isin(missing_organs_series_ids)]\n",
    "    \n",
    "    normal_cases = train_df[(train_df['extravasation_injury']==0) \n",
    "                & (train_df['kidney_healthy']==1)\n",
    "                & (train_df['liver_healthy']==1)\n",
    "                & (train_df['spleen_healthy']==1)\n",
    "                & (train_df['bowel_injury']==0)].series_id.unique()\n",
    "    \n",
    "    ids_to_remove = random.sample(list(normal_cases), len(train_df.series_id.unique())%4)\n",
    "    \n",
    "    train_df = train_df[~train_df['series_id'].isin(ids_to_remove)]\n",
    "    train_dataset = OrganTrainDataset(train_df, transform=transforms_train)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "    \n",
    "    test_df = merged_df[merged_df['patient_id'].isin(test_patients)]\n",
    "    organ_check = test_df.groupby('series_id')['organ'].agg(check_organs)\n",
    "    missing_organs_series_ids = organ_check[~organ_check].index.tolist()\n",
    "    test_df = test_df[~test_df['series_id'].isin(missing_organs_series_ids)]\n",
    "    \n",
    "    test_dataset = OrganTrainDataset(test_df, transform=transforms_valid)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "    \n",
    "    metric_best = np.inf\n",
    "    loss_min = np.inf\n",
    "    \n",
    "    model_device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    model = TimmModelWithDualLSTM(backbone=backbone, pretrained=True)\n",
    "    model.to(model_device)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    init_lr = 1e-4\n",
    "    eta_min = 25e-6\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=init_lr)\n",
    "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs, eta_min=eta_min)\n",
    "    \n",
    "    use_amp = True\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "    # epoch = 1\n",
    "        model.train()\n",
    "    \n",
    "        print(time.ctime(), 'Epoch:', epoch)\n",
    "    \n",
    "        train_loss = []\n",
    "        bowel_binary_loss_list = []\n",
    "        extravasation_binary_loss_list = []\n",
    "        kidney_multi_loss_list = []\n",
    "        liver_multi_loss_list = []\n",
    "        spleen_multi_loss_list = []\n",
    "        \n",
    "        bar = tqdm(train_dataloader)\n",
    "        \n",
    "        for images, targets in bar:\n",
    "            images = images.to(model_device)\n",
    "            targets = targets.to(model_device)\n",
    "        \n",
    "            do_mixup = False\n",
    "            if random.random() < p_mixup:\n",
    "                do_mixup = True\n",
    "                images, targets, targets_mix, lam = mixup(images, targets)\n",
    "        \n",
    "            # print(torch.unique(targets))\n",
    "    \n",
    "            with amp.autocast():\n",
    "                logits_binary, logits_multiclass = model(images)\n",
    "                \n",
    "                # Compute individual losses\n",
    "                liver_multi_loss = liver_multi_criterion(logits_multiclass, targets, model_device)\n",
    "        \n",
    "                bowel_binary_loss = bowel_binary_criterion(logits_binary, targets, model_device)\n",
    "                \n",
    "                extravasation_binary_loss = extravasation_binary_criterion(logits_binary, targets, model_device)\n",
    "                \n",
    "                kidney_multi_loss = kidney_multi_criterion(logits_multiclass, targets, model_device)\n",
    "        \n",
    "                spleen_multi_loss = spleen_multi_criterion(logits_multiclass, targets, model_device)\n",
    "            \n",
    "                # Define your weights\n",
    "                w_bowel = 1.0\n",
    "                w_extravasation = 1.0\n",
    "                w_kidney = 1.0\n",
    "                w_liver = 1.0\n",
    "                w_spleen = 1.0\n",
    "                \n",
    "                # Compute the weighted combination\n",
    "                loss = (w_bowel * bowel_binary_loss +\n",
    "                                 w_extravasation * extravasation_binary_loss +\n",
    "                                 w_kidney * kidney_multi_loss +\n",
    "                                 w_liver * liver_multi_loss +\n",
    "                                 w_spleen * spleen_multi_loss) / (w_bowel + w_extravasation + w_kidney + w_liver + w_spleen)     \n",
    "                # If mixup is performed, adjust the loss\n",
    "                if do_mixup:\n",
    "                    liver_multi_mix = liver_multi_criterion(logits_multiclass, targets_mix, model_device)\n",
    "        \n",
    "                    loss = loss * lam + (w_liver * liver_multi_loss) / (w_liver ) * (1 - lam)\n",
    "        \n",
    "                    bowel_binary_mix = bowel_binary_criterion(logits_binary, targets_mix, model_device)\n",
    "                    \n",
    "                    extravasation_binary_mix = extravasation_binary_criterion(logits_binary, targets_mix, model_device)\n",
    "                    \n",
    "                    kidney_multi_mix = kidney_multi_criterion(logits_multiclass, targets_mix, model_device)\n",
    "                \n",
    "                    spleen_multi_mix = spleen_multi_criterion(logits_multiclass, targets_mix, model_device)\n",
    "                    \n",
    "                    loss = loss * lam + (w_bowel * bowel_binary_loss +\n",
    "                                             w_extravasation * extravasation_binary_loss +\n",
    "                                             w_kidney * kidney_multi_loss +\n",
    "                                             w_liver * liver_multi_loss +\n",
    "                                             w_spleen * spleen_multi_loss) / (w_bowel + w_extravasation + w_kidney + w_liver + w_spleen) * (1 - lam)\n",
    "        \n",
    "            liver_multi_loss_list.append(liver_multi_loss.item()) \n",
    "        \n",
    "            bowel_binary_loss_list.append(bowel_binary_loss.item())\n",
    "            \n",
    "            extravasation_binary_loss_list.append(extravasation_binary_loss.item())\n",
    "            \n",
    "            kidney_multi_loss_list.append(kidney_multi_loss.item()) \n",
    "        \n",
    "            spleen_multi_loss_list.append(spleen_multi_loss.item())\n",
    "        \n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler_cosine.step()\n",
    "        \n",
    "            # if train_loss and bowel_binary_loss and extravasation_binary_loss and kidney_multi_loss and liver_multi_loss and spleen_multi_loss:\n",
    "            bar.set_description(f'Train Loss: {np.mean(train_loss):.4f}')#, Bowel Loss: {np.mean(bowel_binary_loss):.4f}, Ext Loss: {np.mean(extravasation_binary_loss):.4f}, Kidney Loss: {np.mean(kidney_multi_loss):.4f}, Liver Loss: {np.mean(liver_multi_loss):.4f}, Spleen Loss: {np.mean(spleen_multi_loss):.4f}')\n",
    "    \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "        test_loss = []\n",
    "        bowel_binary_loss_list = []\n",
    "        extravasation_binary_loss_list = []\n",
    "        kidney_multi_loss_list = []\n",
    "        liver_multi_loss_list = []\n",
    "        spleen_multi_loss_list = []\n",
    "    \n",
    "        metric = 0 \n",
    "        bar = tqdm(test_dataloader)\n",
    "    \n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for images, targets in bar:\n",
    "                images = images.to(model_device)\n",
    "                targets = targets.to(model_device)\n",
    "    \n",
    "                logits_binary, logits_multiclass = model(images)\n",
    "                            \n",
    "                # Compute individual losses\n",
    "                liver_multi_loss = liver_multi_criterion(logits_multiclass, targets, model_device)\n",
    "        \n",
    "                bowel_binary_loss = bowel_binary_criterion(logits_binary, targets, model_device)\n",
    "                \n",
    "                extravasation_binary_loss = extravasation_binary_criterion(logits_binary, targets, model_device)\n",
    "                \n",
    "                kidney_multi_loss = kidney_multi_criterion(logits_multiclass, targets, model_device)\n",
    "        \n",
    "                spleen_multi_loss = spleen_multi_criterion(logits_multiclass, targets, model_device)\n",
    "            \n",
    "                # Define your weights\n",
    "                w_bowel = 1.0\n",
    "                w_extravasation = 1.0\n",
    "                w_kidney = 1.0\n",
    "                w_liver = 1.0\n",
    "                w_spleen = 1.0\n",
    "                \n",
    "                # Compute the weighted combination\n",
    "                loss = (w_bowel * bowel_binary_loss +\n",
    "                                 w_extravasation * extravasation_binary_loss +\n",
    "                                 w_kidney * kidney_multi_loss +\n",
    "                                 w_liver * liver_multi_loss +\n",
    "                                 w_spleen * spleen_multi_loss) / (w_bowel + w_extravasation + w_kidney + w_liver + w_spleen)     \n",
    "               \n",
    "            liver_multi_loss_list.append(liver_multi_loss.item()) \n",
    "        \n",
    "            bowel_binary_loss_list.append(bowel_binary_loss.item())\n",
    "            \n",
    "            extravasation_binary_loss_list.append(extravasation_binary_loss.item())\n",
    "            \n",
    "            kidney_multi_loss_list.append(kidney_multi_loss.item()) \n",
    "        \n",
    "            spleen_multi_loss_list.append(spleen_multi_loss.item())\n",
    "        \n",
    "            test_loss.append(loss.item())\n",
    "        \n",
    "            # if train_loss and bowel_binary_loss and extravasation_binary_loss and kidney_multi_loss and liver_multi_loss and spleen_multi_loss:\n",
    "            bar.set_description(f'Test Loss: {np.mean(test_loss):.4f}')#, Bowel Loss: {np.mean(bowel_binary_loss):.4f}, Ext Loss: {np.mean(extravasation_binary_loss):.4f}, Kidney Loss: {np.mean(kidney_multi_loss):.4f}, Liver Loss: {np.mean(liver_multi_loss):.4f}, Spleen Loss: {np.mean(spleen_multi_loss):.4f}')\n",
    "    \n",
    "        metric = np.mean(test_loss)\n",
    "        content = f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, valid loss: {np.mean(test_loss):.5f}, metric: {metric:.6f}.'\n",
    "        print(content)\n",
    "    \n",
    "        if metric < metric_best:\n",
    "            print(f'metric_best ({metric_best:.6f} --> {metric:.6f}). Saving model ...')\n",
    "    #             if not DEBUG:\n",
    "            torch.save(model.state_dict(), model_file)\n",
    "            metric_best = metric\n",
    "\n",
    "           # Save Last\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict() if scaler else None,\n",
    "                'score_best': metric_best,\n",
    "            },\n",
    "            model_file.replace('_best', '_last')\n",
    "        )\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77d53c20-8196-496c-a74d-cd96107d9afd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([1, 11])\n",
      "Using 2 GPUs!\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([1, 11])\n",
      "Using 2 GPUs!\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "Using 2 GPUs!\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([1, 11])\n",
      "Using 2 GPUs!\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([2, 11])\n",
      "torch.Size([1, 11])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you've defined all your necessary functions, imports, and setup above...\n",
    "\n",
    "predict_list = []\n",
    "true_list = []\n",
    "\n",
    "for fold in range(5):\n",
    "    \n",
    "    log_dir = 'logs'\n",
    "    model_dir = 'best_lstm_model'\n",
    "    kernel_type ='not_real'\n",
    "    \n",
    "    log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n",
    "    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    \n",
    "    train_patients, test_patients = train_split[fold], test_split[fold]#train_patients, test_patients = next(stratifier.split(y_original_format, y_original_format))\n",
    "    \n",
    "    test_df = merged_df[merged_df['patient_id'].isin(test_patients)]\n",
    "    organ_check = test_df.groupby('series_id')['organ'].agg(check_organs)\n",
    "    missing_organs_series_ids = organ_check[~organ_check].index.tolist()\n",
    "    test_df = test_df[~test_df['series_id'].isin(missing_organs_series_ids)]\n",
    "    \n",
    "    test_dataset = OrganTrainDataset(test_df, transform=transforms_valid)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "    \n",
    "    model_device = torch.device(\"cuda:0\")\n",
    "    model = TimmModelWithDualLSTM(backbone=backbone, pretrained=True)\n",
    "    model.to(model_device)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # Load the best model for this fold\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    true_labels = {\n",
    "        'bowel': [],\n",
    "        'extravasation': [],\n",
    "        'kidney': [],\n",
    "        'liver': [],\n",
    "        'spleen': []\n",
    "    }\n",
    "\n",
    "    predicted_labels = {\n",
    "        'bowel': [],\n",
    "        'extravasation': [],\n",
    "        'kidney': [],\n",
    "        'liver': [],\n",
    "        'spleen': []\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_dataloader:\n",
    "            images = images.to(model_device)\n",
    "            targets = targets.to(model_device)\n",
    "\n",
    "            print(targets.shape)\n",
    "            logits_binary, logits_multiclass = model(images)\n",
    "            \n",
    "            # This needs adjustment based on your actual tensor shapes and meanings:\n",
    "            true_labels['bowel'].extend(targets[:, 0].cpu().numpy())\n",
    "            true_labels['extravasation'].extend(targets[:, 1].cpu().numpy())\n",
    "            true_labels['kidney'].extend(targets[:, 2:5].cpu().numpy())\n",
    "            true_labels['liver'].extend(targets[:, 5:8].cpu().numpy())\n",
    "            true_labels['spleen'].extend(targets[:, 8:11].cpu().numpy())\n",
    "\n",
    "            # Binary classifications\n",
    "            predicted_labels['bowel'].extend((torch.sigmoid(logits_binary[:, 0]) >= 0.5).long().cpu().numpy())\n",
    "            predicted_labels['extravasation'].extend((torch.sigmoid(logits_binary[:, 1]) >= 0.5).long().cpu().numpy())\n",
    "            \n",
    "            # Multi-class classifications\n",
    "            predicted_labels['kidney'].extend(torch.argmax(logits_multiclass[:, :3], dim=1).cpu().numpy())  # Assuming 3 classes for kidney\n",
    "            predicted_labels['liver'].extend(torch.argmax(logits_multiclass[:, 3:6], dim=1).cpu().numpy())  # Assuming 3 classes for liver\n",
    "            predicted_labels['spleen'].extend(torch.argmax(logits_multiclass[:, 6:], dim=1).cpu().numpy())  # Assuming 3 classes for spleen\n",
    "\n",
    "    # print(f\"Fold {fold} Results:\")\n",
    "    # for organ in true_labels.keys():\n",
    "    #     report = classification_report(true_labels[organ], predicted_labels[organ], target_names=[f'Not {organ}', organ])\n",
    "    #     print(f\"Classification Report for {organ}:\")\n",
    "    #     print(report)\n",
    "    #     print('-' * 50)\n",
    "\n",
    "    predict_list.append(predicted_labels)\n",
    "    true_list.append(true_labels)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65ab5833-1e7a-4ea8-b70d-f14578a0ee5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_list[0]['spleen']# true_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3079fafd-c725-4e95-9d99-c8dd30546dc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([0., 1., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_list[0]['spleen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36395204-e78e-4702-8941-a693d2794b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "\n",
    "train_patients, test_patients = train_split[fold], test_split[fold]#train_patients, test_patients = next(stratifier.split(y_original_format, y_original_format))\n",
    "\n",
    "test_df = merged_df[merged_df['patient_id'].isin(test_patients)]\n",
    "organ_check = test_df.groupby('series_id')['organ'].agg(check_organs)\n",
    "missing_organs_series_ids = organ_check[~organ_check].index.tolist()\n",
    "test_df = test_df[~test_df['series_id'].isin(missing_organs_series_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4154f584-3f7c-47bb-a449-c8dbf5d0ab45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>organ</th>\n",
       "      <th>image_path</th>\n",
       "      <th>image_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>bowel_injury</th>\n",
       "      <th>extravasation_injury</th>\n",
       "      <th>kidney_healthy</th>\n",
       "      <th>kidney_low</th>\n",
       "      <th>kidney_high</th>\n",
       "      <th>liver_healthy</th>\n",
       "      <th>liver_low</th>\n",
       "      <th>liver_high</th>\n",
       "      <th>spleen_healthy</th>\n",
       "      <th>spleen_low</th>\n",
       "      <th>spleen_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>9</td>\n",
       "      <td>liver</td>\n",
       "      <td>series_image_split/9/liver/1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>9</td>\n",
       "      <td>liver</td>\n",
       "      <td>series_image_split/9/liver/4.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>9</td>\n",
       "      <td>liver</td>\n",
       "      <td>series_image_split/9/liver/7.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>9</td>\n",
       "      <td>liver</td>\n",
       "      <td>series_image_split/9/liver/10.jpg</td>\n",
       "      <td>10</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>9</td>\n",
       "      <td>liver</td>\n",
       "      <td>series_image_split/9/liver/13.jpg</td>\n",
       "      <td>13</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555091</th>\n",
       "      <td>64423</td>\n",
       "      <td>bowel</td>\n",
       "      <td>series_image_split/64423/bowel/100.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>2715</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555092</th>\n",
       "      <td>64423</td>\n",
       "      <td>bowel</td>\n",
       "      <td>series_image_split/64423/bowel/103.jpg</td>\n",
       "      <td>103</td>\n",
       "      <td>2715</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555093</th>\n",
       "      <td>64423</td>\n",
       "      <td>bowel</td>\n",
       "      <td>series_image_split/64423/bowel/106.jpg</td>\n",
       "      <td>106</td>\n",
       "      <td>2715</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555094</th>\n",
       "      <td>64423</td>\n",
       "      <td>bowel</td>\n",
       "      <td>series_image_split/64423/bowel/109.jpg</td>\n",
       "      <td>109</td>\n",
       "      <td>2715</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555095</th>\n",
       "      <td>64423</td>\n",
       "      <td>bowel</td>\n",
       "      <td>series_image_split/64423/bowel/112.jpg</td>\n",
       "      <td>112</td>\n",
       "      <td>2715</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5190 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        series_id  organ                              image_path  image_name  \\\n",
       "180             9  liver        series_image_split/9/liver/1.jpg           1   \n",
       "181             9  liver        series_image_split/9/liver/4.jpg           4   \n",
       "182             9  liver        series_image_split/9/liver/7.jpg           7   \n",
       "183             9  liver       series_image_split/9/liver/10.jpg          10   \n",
       "184             9  liver       series_image_split/9/liver/13.jpg          13   \n",
       "...           ...    ...                                     ...         ...   \n",
       "555091      64423  bowel  series_image_split/64423/bowel/100.jpg         100   \n",
       "555092      64423  bowel  series_image_split/64423/bowel/103.jpg         103   \n",
       "555093      64423  bowel  series_image_split/64423/bowel/106.jpg         106   \n",
       "555094      64423  bowel  series_image_split/64423/bowel/109.jpg         109   \n",
       "555095      64423  bowel  series_image_split/64423/bowel/112.jpg         112   \n",
       "\n",
       "        patient_id  bowel_injury  extravasation_injury  kidney_healthy  \\\n",
       "180            403             0                     0               1   \n",
       "181            403             0                     0               1   \n",
       "182            403             0                     0               1   \n",
       "183            403             0                     0               1   \n",
       "184            403             0                     0               1   \n",
       "...            ...           ...                   ...             ...   \n",
       "555091        2715             0                     0               1   \n",
       "555092        2715             0                     0               1   \n",
       "555093        2715             0                     0               1   \n",
       "555094        2715             0                     0               1   \n",
       "555095        2715             0                     0               1   \n",
       "\n",
       "        kidney_low  kidney_high  liver_healthy  liver_low  liver_high  \\\n",
       "180              0            0              1          0           0   \n",
       "181              0            0              1          0           0   \n",
       "182              0            0              1          0           0   \n",
       "183              0            0              1          0           0   \n",
       "184              0            0              1          0           0   \n",
       "...            ...          ...            ...        ...         ...   \n",
       "555091           0            0              1          0           0   \n",
       "555092           0            0              1          0           0   \n",
       "555093           0            0              1          0           0   \n",
       "555094           0            0              1          0           0   \n",
       "555095           0            0              1          0           0   \n",
       "\n",
       "        spleen_healthy  spleen_low  spleen_high  \n",
       "180                  1           0            0  \n",
       "181                  1           0            0  \n",
       "182                  1           0            0  \n",
       "183                  1           0            0  \n",
       "184                  1           0            0  \n",
       "...                ...         ...          ...  \n",
       "555091               1           0            0  \n",
       "555092               1           0            0  \n",
       "555093               1           0            0  \n",
       "555094               1           0            0  \n",
       "555095               1           0            0  \n",
       "\n",
       "[5190 rows x 16 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e94de-4d54-4ef3-9293-97f5e4158164",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
